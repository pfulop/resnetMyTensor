{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs, kernel_size, filters, name, strides=(2, 2)):\n",
    "    \n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    x = tf.layers.conv2d(inputs,filters1, (1, 1), padding=\"same\"\n",
    "                        ,name=\"{}conv1\".format(name))\n",
    "    x = tf.layers.batch_normalization(x ,name=\"{}batch1\".format(name))\n",
    "    x = tf.nn.relu(x ,name=\"{}relu1\".format(name))\n",
    "\n",
    "    x = tf.layers.conv2d(x,filters2, kernel_size, strides=strides, padding=\"same\"\n",
    "                         ,name=\"{}conv2\".format(name))\n",
    "    x = tf.layers.batch_normalization(x ,name=\"{}batch2\".format(name))\n",
    "    x = tf.nn.relu(x,name=\"{}relu2\".format(name))\n",
    "\n",
    "    x = tf.layers.conv2d(x,filters3, (1, 1), padding=\"same\"\n",
    "                          ,name=\"{}conv3\".format(name))\n",
    "    x = tf.layers.batch_normalization(x ,name=\"{}batch3\".format(name))\n",
    "\n",
    "    shortcut = tf.layers.conv2d(inputs, filters3, (1, 1), strides=strides,\n",
    "                               name=\"{}conv4\".format(name))\n",
    "    shortcut = tf.layers.batch_normalization(shortcut ,name=\"{}batch4\".format(name))\n",
    "\n",
    "    x = x + shortcut\n",
    "    x = tf.nn.relu(x,name=\"{}relu3\".format(name))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(inputs, kernel_size, filters, name):\n",
    "    \n",
    "    filters1, filters2, filters3 = filters\n",
    "    \n",
    "    x = tf.layers.conv2d(inputs,filters1, (1, 1), padding=\"same\" \n",
    "                         ,name=\"{}identiy_conv1\".format(name))\n",
    "    x = tf.layers.batch_normalization(x,name=\"{}identiy_batch1\".format(name))\n",
    "    x = tf.nn.relu(x,name=\"{}identiy_relu1\".format(name))\n",
    "\n",
    "    x = tf.layers.conv2d(x,filters2, kernel_size, padding=\"same\"\n",
    "                          ,name=\"{}identiy_conv2\".format(name))\n",
    "    x = tf.layers.batch_normalization(x,name=\"{}identiy_batch2\".format(name))\n",
    "    x = tf.nn.relu(x,name=\"{}identiy_relu2\".format(name))\n",
    "\n",
    "    x = tf.layers.conv2d(x,filters3, (1, 1), padding=\"same\"\n",
    "                          ,name=\"{}identiy_conv3\".format(name))\n",
    "    x = tf.layers.batch_normalization(x,name=\"{}identiy_batch3\".format(name))\n",
    "\n",
    "    x = x + inputs\n",
    "    x = tf.nn.relu(x,name=\"{}identiy_relu3\".format(name))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create(x, y, n_classes):\n",
    "    x = tf.layers.conv2d(x, 64, 7, padding=\"same\")\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.max_pooling2d(x, (3,3), (2,2))\n",
    "    \n",
    "    x = conv_block(x, 3, [64, 64, 256], strides=(1,1), name=\"first\")\n",
    "    x = identity_block(x, 3, [64, 64, 256], name=\"first\")\n",
    "    x = identity_block(x, 3, [64, 64, 256], name=\"second\")\n",
    "    \n",
    "    x = conv_block(x, 3, [128, 128, 512], name=\"second\")\n",
    "    for i in range(0,2): x = identity_block(x, 3, [128, 128, 512], name=\"third{}\".format(i))\n",
    "    x = conv_block(x, 3, [256, 256, 1024], name=\"third\")\n",
    "    for i in range(0,4): x = identity_block(x, 3, [256, 256, 1024], name=\"fourth{}\".format(i))\n",
    "    \n",
    "    \n",
    "    x = conv_block(x, 3, [512, 512, 2048], strides=(1,1), name=\"fifth\")\n",
    "    x = identity_block(x, 3, [512, 512, 2048], name=\"sixth\")\n",
    "    x = identity_block(x, 3, [512, 512, 2048],name=\"seventh\")\n",
    "    \n",
    "    x = tf.layers.average_pooling2d(x,(7, 7), (7,7))\n",
    "    x = tf.reshape(x, [-1, 4 * 4 * 2048])\n",
    "    x = tf.layers.dense(\n",
    "                    x,\n",
    "                    n_classes,\n",
    "                )\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "    correct_pred = tf.equal(tf.argmax(x, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    return x, loss, optimizer, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Inputs import *\n",
    "from prep import prepare_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/backman/PycharmProjects/resnet/Inputs.py:37: Dataset.from_tensor_slices (from tensorflow.contrib.data.python.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.from_tensor_slices()`.\n",
      "WARNING:tensorflow:From /home/backman/PycharmProjects/resnet/Inputs.py:40: calling Dataset.map (from tensorflow.contrib.data.python.ops.dataset_ops) with num_threads is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\n",
      "WARNING:tensorflow:From /home/backman/PycharmProjects/resnet/Inputs.py:40: calling Dataset.map (from tensorflow.contrib.data.python.ops.dataset_ops) with output_buffer_size is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, valid_images, valid_labels, n_classes = prepare_data('/run/media/backman/yay/dogbreed/sample/')\n",
    "train_inputs = Inputs(train_images, train_labels, n_classes, batch_size=2, shuffle=True)\n",
    "valid_inputs = Inputs(valid_images, valid_labels, n_classes, name=\"valid\", batch_size=10)\n",
    "with tf.device('/cpu:0'):\n",
    "    iterator_train = train_inputs.generate_iterator()\n",
    "    iterator_valid = valid_inputs.generate_iterator()\n",
    "\n",
    "train_batches_per_epoch = int(np.floor(train_inputs.size / 2))\n",
    "valid_batches_per_epoch = int(np.floor(valid_inputs.size / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, None, None, 3])\n",
    "y = tf.placeholder(tf.float32, None)\n",
    "model, loss, optimizer, accuracy = create(x,y, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_train = iterator_train.get_next()\n",
    "next_valid = iterator_valid.get_next()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    sess.run(iterator_train.initializer)    \n",
    "    for i in range(train_batches_per_epoch):\n",
    "        batch_data, batch_labels = sess.run(next_train)\n",
    "        feed_dict = {x: batch_data, y: batch_labels}\n",
    "        res = sess.run([optimizer, loss, accuracy], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
